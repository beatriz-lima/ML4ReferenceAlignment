{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML4ReferenceAlignment tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose:** generating a partial reference alignment for ontology matching tool evaluation.  \n",
    "**Methods**: training a machine learning model to learn what a correct alignment looks like, by using the outputs of the ontology matching tools that have participated in previous OAEI tracks as *features* and the quality  manual reference alignment available for each of those tracks as *target*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents \n",
    "\n",
    "0. [Data](#data)\n",
    "1. [Preprocessing](#preprocessing)\n",
    "2. [Training the models](#training)\n",
    "3. [Evaluating the models](#evaluation)\n",
    "4. [Classifying a new instance](#classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Data <a id = \"data\"></a>\n",
    "\n",
    "The data used in these experiments is available in the OAEI website. To train and test the models, you will need two types of datasets:\n",
    " -  alignments outputted by the participating OM tools in the OAEI competition (rdf files); we will have one rdf file per participating tool, for each track. Here we have the links for the 2019 results (originally used in this work).\n",
    "\t - Anatomy track: http://oaei.ontologymatching.org/2019/results/anatomy/oaei2019-anatomy-alignments.zip\n",
    "\t - LargeBioMed track: http://www.cs.ox.ac.uk/isg/projects/SEALS/oaei/2019/results/largebio-results-2019.zip\n",
    "\t - Conference track: http://oaei.ontologymatching.org/2019/conference/data/conference2019results.zip\n",
    "     \n",
    "- the reference alignment provided for each track\n",
    "    - Anatomy track: http://oaei.ontologymatching.org/2019/anatomy/anatomy-dataset.zip (reference.rdf)\n",
    "    - LargeBioMed track: http://www.cs.ox.ac.uk/isg/projects/SEALS/oaei/2019/LargeBio_dataset_oaei2019.zip (oaei_ont1ont2_UMLS_mappings_with_flagged_repairs.rdf files)\n",
    "\t- Conference track: http://oaei.ontologymatching.org/2019/conference/data/reference-alignment.zip\n",
    "\n",
    "Despite this, one could virtually use any other dataset as long as there is a reference available. It's important to keep in mind that the higher the quality of the reference alignment, the higher the chances of developing a well performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://i.pinimg.com/originals/4d/23/43/4d2343fe4228140e84af25cddd809659.jpg width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing <a id = \"preprocessing\"></a>\n",
    "\n",
    "In order to feed our machine learning models with training examples, we want to have this kind of data structure:\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/43668147/99803653-bf6e4980-2b31-11eb-81d6-45aff638e47c.png)\n",
    "\n",
    "However, a processing step is needed to obtain this kind of data structure from the raw outputs of the tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as u\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load the outputs of the participating tools (features) and the reference alingnment (target)\n",
    "\n",
    "Extract fields *entity 1*, *entity 2*, *measure* using the method ```extract_mappings()``` (in utils.py).\n",
    "\n",
    "In the case where the track contains more than two ontologies (e.g. LargeBio and Conference datasets) the process of extracting the mappings produced by each tool is multiplied by the number of possible alignments between the ontologies available. In this case, a function such as ```read_rdf()``` (in utils.py) could be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD ANATOMY\n",
    "an_dir = os.path.join(\"data\",\"anatomy-2019\")\n",
    "an_agm = u.extract_mappings(os.path.join(an_dir, \"AGM.rdf\"))\n",
    "an_aml = u.extract_mappings(os.path.join(an_dir, \"AML.rdf\"))\n",
    "an_dome = u.extract_mappings(os.path.join(an_dir, \"DOME.rdf\"))\n",
    "an_fcamap = u.extract_mappings(os.path.join(an_dir, \"FCAMap-KG.rdf\"))\n",
    "an_logmap = u.extract_mappings(os.path.join(an_dir, \"LogMap.rdf\"))\n",
    "an_logmapbio = u.extract_mappings(os.path.join(an_dir, \"LogMapBio.rdf\"))\n",
    "an_logmaplt = u.extract_mappings(os.path.join(an_dir, \"LogMapLt.rdf\"))\n",
    "an_pomappp = u.extract_mappings(os.path.join(an_dir, \"POMAP++.rdf\"))\n",
    "an_wiktionary = u.extract_mappings(os.path.join(an_dir, \"Wiktionary.rdf\"))  \n",
    "\n",
    "an_tools = {\n",
    "        \"agm\": an_agm,\n",
    "        \"aml\": an_aml,\n",
    "        \"dome\": an_dome,\n",
    "        \"fcamap\": an_fcamap,\n",
    "        \"logmap\": an_logmap,\n",
    "        \"logmapbio\": an_logmapbio,\n",
    "        \"logmaplt\": an_logmaplt,\n",
    "        \"pomap++\": an_pomappp,\n",
    "        \"wiktionary\": an_wiktionary,\n",
    "    }\n",
    "\n",
    "# Reference\n",
    "df_an_ref = u.extract_mappings(os.path.join(an_dir, \"reference.rdf\"), is_ref=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n"
     ]
    }
   ],
   "source": [
    "# LOAD LARGEBIO (FMA-NCI)\n",
    "lb_measures = [\n",
    "    'measure_agm',\n",
    "    'measure_aml',\n",
    "    'measure_dome',\n",
    "    'measure_fcamap',\n",
    "    'measure_logmap',\n",
    "    'measure_logmapbio',\n",
    "    'measure_logmaplt',\n",
    "    'measure_pomap++',\n",
    "    'measure_wiktionary'\n",
    "]\n",
    "\n",
    "lb_data_path = os.path.join(\"data\", \"largebio-2019\")\n",
    "\n",
    "#Data and reference\n",
    "df_lb, df_lb_ref = u.read_rdf(ont1='fma', \n",
    "                              ont2='nci', \n",
    "                              measures=lb_measures, \n",
    "                              track='largebio', \n",
    "                              data_path=lb_data_path, \n",
    "                              ref_path= os.path.join(lb_data_path, \"oaei_fma_nci_mappings_with_flagged_repairs.rdf\"),\n",
    "                              data_processed_path=os.path.join(lb_data_path,\"data_lb.csv\"), \n",
    "                              ref_processed_path=os.path.join(lb_data_path,\"ref_lb.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Merge all data into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Antomy\n",
    "df_an = u.merge_mappings(an_tools)\n",
    "df_an = df_an.merge(df_an_ref, how='outer',on=[\"entity1\", \"entity2\"])\n",
    "df_an.rename(columns={\"measure\": \"label\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Largebio\n",
    "df_lb = df_lb.merge(df_lb_ref, how='outer',on=[\"entity1\", \"entity2\"])\n",
    "df_lb.rename(columns={\"measure\": \"label\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Fill in the missing values - negative sampling\n",
    "\n",
    "Right now the data has some missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity1</th>\n",
       "      <th>entity2</th>\n",
       "      <th>measure_agm</th>\n",
       "      <th>measure_aml</th>\n",
       "      <th>measure_dome</th>\n",
       "      <th>measure_fcamap</th>\n",
       "      <th>measure_logmap</th>\n",
       "      <th>measure_logmapbio</th>\n",
       "      <th>measure_logmaplt</th>\n",
       "      <th>measure_pomap++</th>\n",
       "      <th>measure_wiktionary</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mouse.owl#genid1728</td>\n",
       "      <td>http://human.owl#NCI_C33170</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mouse.owl#MA_0000342</td>\n",
       "      <td>http://human.owl#genid4462</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mouse.owl#genid1416</td>\n",
       "      <td>http://human.owl#genid4717</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mouse.owl#MA_0001235</td>\n",
       "      <td>http://human.owl#genid1248</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mouse.owl#MA_0001052</td>\n",
       "      <td>http://human.owl#NCI_C33187</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.846</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       entity1                      entity2  measure_agm  \\\n",
       "0   http://mouse.owl#genid1728  http://human.owl#NCI_C33170          1.0   \n",
       "1  http://mouse.owl#MA_0000342   http://human.owl#genid4462          1.0   \n",
       "2   http://mouse.owl#genid1416   http://human.owl#genid4717          1.0   \n",
       "3  http://mouse.owl#MA_0001235   http://human.owl#genid1248          1.0   \n",
       "4  http://mouse.owl#MA_0001052  http://human.owl#NCI_C33187          1.0   \n",
       "\n",
       "   measure_aml  measure_dome  measure_fcamap  measure_logmap  \\\n",
       "0          NaN           NaN             NaN             NaN   \n",
       "1          NaN           NaN             NaN             NaN   \n",
       "2          NaN           NaN             NaN             NaN   \n",
       "3          NaN           NaN             NaN             NaN   \n",
       "4        0.846           NaN             NaN             NaN   \n",
       "\n",
       "   measure_logmapbio  measure_logmaplt  measure_pomap++  measure_wiktionary  \\\n",
       "0                NaN               NaN              NaN                 NaN   \n",
       "1                NaN               NaN              NaN                 NaN   \n",
       "2                NaN               NaN              NaN                 NaN   \n",
       "3                NaN               NaN              NaN                 NaN   \n",
       "4               0.96               1.0              1.0            0.166667   \n",
       "\n",
       "   label  \n",
       "0    NaN  \n",
       "1    NaN  \n",
       "2    NaN  \n",
       "3    NaN  \n",
       "4    1.0  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_an.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because we outter merged all of the tools outputs and not all tools contain the same mappings. \n",
    "We also have missing values in the classification label, since the reference alignment only encompasses positives examples. Thus, by filling in the missing values with label 0, we proceed to create negative examples, i.e. mappings that would be classified as incorrect by the tools that ommitted those mappings in the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity1</th>\n",
       "      <th>entity2</th>\n",
       "      <th>measure_agm</th>\n",
       "      <th>measure_aml</th>\n",
       "      <th>measure_dome</th>\n",
       "      <th>measure_fcamap</th>\n",
       "      <th>measure_logmap</th>\n",
       "      <th>measure_logmapbio</th>\n",
       "      <th>measure_logmaplt</th>\n",
       "      <th>measure_pomap++</th>\n",
       "      <th>measure_wiktionary</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://bioontology.org/projects/ontologies/fma...</td>\n",
       "      <td>http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://bioontology.org/projects/ontologies/fma...</td>\n",
       "      <td>http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://bioontology.org/projects/ontologies/fma...</td>\n",
       "      <td>http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://bioontology.org/projects/ontologies/fma...</td>\n",
       "      <td>http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://bioontology.org/projects/ontologies/fma...</td>\n",
       "      <td>http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.73</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14404</th>\n",
       "      <td>http://bioontology.org/projects/ontologies/fma...</td>\n",
       "      <td>http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14405</th>\n",
       "      <td>http://bioontology.org/projects/ontologies/fma...</td>\n",
       "      <td>http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14406</th>\n",
       "      <td>http://bioontology.org/projects/ontologies/fma...</td>\n",
       "      <td>http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14407</th>\n",
       "      <td>http://bioontology.org/projects/ontologies/fma...</td>\n",
       "      <td>http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14408</th>\n",
       "      <td>http://bioontology.org/projects/ontologies/fma...</td>\n",
       "      <td>http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14409 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 entity1  \\\n",
       "0      http://bioontology.org/projects/ontologies/fma...   \n",
       "1      http://bioontology.org/projects/ontologies/fma...   \n",
       "2      http://bioontology.org/projects/ontologies/fma...   \n",
       "3      http://bioontology.org/projects/ontologies/fma...   \n",
       "4      http://bioontology.org/projects/ontologies/fma...   \n",
       "...                                                  ...   \n",
       "14404  http://bioontology.org/projects/ontologies/fma...   \n",
       "14405  http://bioontology.org/projects/ontologies/fma...   \n",
       "14406  http://bioontology.org/projects/ontologies/fma...   \n",
       "14407  http://bioontology.org/projects/ontologies/fma...   \n",
       "14408  http://bioontology.org/projects/ontologies/fma...   \n",
       "\n",
       "                                                 entity2  measure_agm  \\\n",
       "0      http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...          1.0   \n",
       "1      http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...          1.0   \n",
       "2      http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...          1.0   \n",
       "3      http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...          1.0   \n",
       "4      http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...          1.0   \n",
       "...                                                  ...          ...   \n",
       "14404  http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...          0.0   \n",
       "14405  http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...          0.0   \n",
       "14406  http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...          0.0   \n",
       "14407  http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...          0.0   \n",
       "14408  http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...          0.0   \n",
       "\n",
       "       measure_aml  measure_dome  measure_fcamap  measure_logmap  \\\n",
       "0           0.0000           0.0             0.0            0.00   \n",
       "1           0.0000           0.0             0.0            0.00   \n",
       "2           0.0000           0.0             0.0            0.00   \n",
       "3           0.0000           0.0             0.0            0.00   \n",
       "4           0.9801           0.0             1.0            0.73   \n",
       "...            ...           ...             ...             ...   \n",
       "14404       0.0000           0.0             0.0            0.00   \n",
       "14405       0.0000           0.0             0.0            0.00   \n",
       "14406       0.0000           0.0             0.0            0.00   \n",
       "14407       0.0000           0.0             0.0            0.00   \n",
       "14408       0.0000           0.0             0.0            0.00   \n",
       "\n",
       "       measure_logmapbio  measure_logmaplt  measure_pomap++  \\\n",
       "0                   0.00               0.0              0.0   \n",
       "1                   0.00               0.0              0.0   \n",
       "2                   0.00               0.0              0.0   \n",
       "3                   0.00               0.0              0.0   \n",
       "4                   0.73               1.0              0.0   \n",
       "...                  ...               ...              ...   \n",
       "14404               0.00               0.0              0.0   \n",
       "14405               0.00               0.0              0.0   \n",
       "14406               0.00               0.0              0.0   \n",
       "14407               0.00               0.0              0.0   \n",
       "14408               0.00               0.0              0.0   \n",
       "\n",
       "       measure_wiktionary  label  \n",
       "0                     0.0   0.00  \n",
       "1                     0.0   0.00  \n",
       "2                     0.0   0.00  \n",
       "3                     0.0   0.00  \n",
       "4                     1.0   0.57  \n",
       "...                   ...    ...  \n",
       "14404                 0.0   0.41  \n",
       "14405                 0.0   0.41  \n",
       "14406                 0.0   0.34  \n",
       "14407                 0.0   0.45  \n",
       "14408                 0.0   0.38  \n",
       "\n",
       "[14409 rows x 12 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing values\n",
    "df_an.fillna(0)\n",
    "df_lb.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Make the data binary\n",
    "\n",
    "In our experiments, we found that the models that were fed with binary data showed the best results. One possible explanation for this is that the different tools use different score thresholds for selecting mappings to include in the final output, which means that a given score could mean a correct mapping for a tool but an incorrect one to another. Having no appropriate way to normalize these scores, we decided to make the data binary. Thus, ``1`` represents a correct mapping according to that OM system, and ``0`` represents an incorrect mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make data binary\n",
    "X_bins_an = u.bin_features(df_an, 0,1)\n",
    "X_bins_lb = u.bin_features(df_lb,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity1</th>\n",
       "      <th>entity2</th>\n",
       "      <th>measure_agm</th>\n",
       "      <th>measure_aml</th>\n",
       "      <th>measure_dome</th>\n",
       "      <th>measure_fcamap</th>\n",
       "      <th>measure_logmap</th>\n",
       "      <th>measure_logmapbio</th>\n",
       "      <th>measure_logmaplt</th>\n",
       "      <th>measure_pomap++</th>\n",
       "      <th>measure_wiktionary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://bioontology.org/projects/ontologies/fma...</td>\n",
       "      <td>http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://bioontology.org/projects/ontologies/fma...</td>\n",
       "      <td>http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://bioontology.org/projects/ontologies/fma...</td>\n",
       "      <td>http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://bioontology.org/projects/ontologies/fma...</td>\n",
       "      <td>http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://bioontology.org/projects/ontologies/fma...</td>\n",
       "      <td>http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14404</th>\n",
       "      <td>http://bioontology.org/projects/ontologies/fma...</td>\n",
       "      <td>http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14405</th>\n",
       "      <td>http://bioontology.org/projects/ontologies/fma...</td>\n",
       "      <td>http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14406</th>\n",
       "      <td>http://bioontology.org/projects/ontologies/fma...</td>\n",
       "      <td>http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14407</th>\n",
       "      <td>http://bioontology.org/projects/ontologies/fma...</td>\n",
       "      <td>http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14408</th>\n",
       "      <td>http://bioontology.org/projects/ontologies/fma...</td>\n",
       "      <td>http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14409 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 entity1  \\\n",
       "0      http://bioontology.org/projects/ontologies/fma...   \n",
       "1      http://bioontology.org/projects/ontologies/fma...   \n",
       "2      http://bioontology.org/projects/ontologies/fma...   \n",
       "3      http://bioontology.org/projects/ontologies/fma...   \n",
       "4      http://bioontology.org/projects/ontologies/fma...   \n",
       "...                                                  ...   \n",
       "14404  http://bioontology.org/projects/ontologies/fma...   \n",
       "14405  http://bioontology.org/projects/ontologies/fma...   \n",
       "14406  http://bioontology.org/projects/ontologies/fma...   \n",
       "14407  http://bioontology.org/projects/ontologies/fma...   \n",
       "14408  http://bioontology.org/projects/ontologies/fma...   \n",
       "\n",
       "                                                 entity2  measure_agm  \\\n",
       "0      http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...            1   \n",
       "1      http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...            1   \n",
       "2      http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...            1   \n",
       "3      http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...            1   \n",
       "4      http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...            1   \n",
       "...                                                  ...          ...   \n",
       "14404  http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...            0   \n",
       "14405  http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...            0   \n",
       "14406  http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...            0   \n",
       "14407  http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...            0   \n",
       "14408  http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus...            0   \n",
       "\n",
       "       measure_aml  measure_dome  measure_fcamap  measure_logmap  \\\n",
       "0                0             0               0               0   \n",
       "1                0             0               0               0   \n",
       "2                0             0               0               0   \n",
       "3                0             0               0               0   \n",
       "4                1             0               1               1   \n",
       "...            ...           ...             ...             ...   \n",
       "14404            0             0               0               0   \n",
       "14405            0             0               0               0   \n",
       "14406            0             0               0               0   \n",
       "14407            0             0               0               0   \n",
       "14408            0             0               0               0   \n",
       "\n",
       "       measure_logmapbio  measure_logmaplt  measure_pomap++  \\\n",
       "0                      0                 0                0   \n",
       "1                      0                 0                0   \n",
       "2                      0                 0                0   \n",
       "3                      0                 0                0   \n",
       "4                      1                 1                0   \n",
       "...                  ...               ...              ...   \n",
       "14404                  0                 0                0   \n",
       "14405                  0                 0                0   \n",
       "14406                  0                 0                0   \n",
       "14407                  0                 0                0   \n",
       "14408                  0                 0                0   \n",
       "\n",
       "       measure_wiktionary  \n",
       "0                       0  \n",
       "1                       0  \n",
       "2                       0  \n",
       "3                       0  \n",
       "4                       1  \n",
       "...                   ...  \n",
       "14404                   0  \n",
       "14405                   0  \n",
       "14406                   0  \n",
       "14407                   0  \n",
       "14408                   0  \n",
       "\n",
       "[14409 rows x 11 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Labelled dataset\n",
    "Xy_bins_an = X_bins_an.copy()\n",
    "Xy_bins_lb = X_bins_lb.copy()\n",
    "\n",
    "#Unlabelled dataset for training\n",
    "X_bins_an.drop('label', axis=1)\n",
    "X_bins_lb.drop('label', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Exploratory data analysis\n",
    "\n",
    "Here we have the number of positive and negative examples. The data imbalance will be tackled later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1960\n",
       "1    1516\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NUMBER OF NEGATIVE AND POSITIVE EXAMPLES\n",
    "Xy_bins_an['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    11403\n",
       "1     3006\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy_bins_lb['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the correlation between each feature and the label, one could get a glimpse on the features' importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measure_agm          -0.645437\n",
      "measure_aml           0.899889\n",
      "measure_dome          0.675455\n",
      "measure_fcamap        0.687745\n",
      "measure_logmap        0.791106\n",
      "measure_logmapbio     0.809404\n",
      "measure_logmaplt      0.738062\n",
      "measure_pomap++       0.818384\n",
      "measure_wiktionary    0.742290\n",
      "label                 1.000000\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(Xy_bins_an.corr()['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measure_agm          -0.326691\n",
      "measure_aml           0.783928\n",
      "measure_dome          0.663104\n",
      "measure_fcamap        0.640909\n",
      "measure_logmap        0.779242\n",
      "measure_logmapbio     0.755499\n",
      "measure_logmaplt      0.672930\n",
      "measure_pomap++      -0.230332\n",
      "measure_wiktionary    0.687324\n",
      "label                 1.000000\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(Xy_bins_lb.corr()['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training the models <a id = \"training\"></a>\n",
    "\n",
    "For the training, tuning and testing of the models, we apply a mega function, ```utils.train_and_eval()```. This function takes as parameters:\n",
    "   - **cross_tuples**: A list of tuples with the following shape: \n",
    "        ```(*[Training DataFrames], *[Testing DataFrames], name : string)```\n",
    "   - **classifiers**: list of classifier classes\n",
    "   - **classifier_kwargs**: list of dictionaries that will be used as keyword arguments for the classifier. If the kwargs includes a key 'param_grid' with a dictionary of value ranges, the optimum hyperparameters will be searched for using a GridSearch.\n",
    "   - **missing_feature_strategy**: Either intersection or substitution. Intersection will remove the non-common features. Substitution will substitute the prediction of the missing tool with a 0.\n",
    "   - **undersample**: Boolean. Indicates whether to try undersampling.\n",
    "   - **save**: String. Path to which the pickled dataframe will be saved. Enabling this function might be useful since the dataframe contains information on the objects of the classifiers (beta coefficients, weights, etc.), which may be used for further analysis. \n",
    "   - **save_rate**: The rate of save, in number of models trained. Every N models, the results are saved.\n",
    "   \n",
    "***Warning: this step might take some time - use the 'results_anatomy.pkl' file for a quick overview of the resulting models.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case we will be training the models using the anatomy dataset \n",
    "# and testing them with the largebio dataset.\n",
    "# Notice that the features (tools used) in the two datasets are the same - this is mandatory\n",
    "# If the participating tools in each of the tracks are not the same, \n",
    "# we could only consider the intersection of those tools.\n",
    "\n",
    "cross_tuples=[([Xy_bins_an], [Xy_bins_lb], \"anatomy-lb\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE CLASSIFIERS & ARGS\n",
    "classifiers = [\n",
    "    RandomForestClassifier,\n",
    "    KNeighborsClassifier,\n",
    "    DecisionTreeClassifier,\n",
    "    MLPClassifier,\n",
    "    GaussianNB,\n",
    "    GradientBoostingClassifier,\n",
    "    LogisticRegression,\n",
    "    AdaBoostClassifier\n",
    "]\n",
    "\n",
    "classifier_kwargs = [\n",
    "    {\"param_grid\": {'n_estimators': list(range(50,250,50)) , 'criterion': ['gini', 'entropy']}},\n",
    "    {\"param_grid\": {'n_neighbors': list(range(1,7)), 'p': [1,2]}},\n",
    "    {\"param_grid\": {'criterion': ['gini', 'entropy']}},\n",
    "    {\"param_grid\": {'hidden_layer_sizes':[(10,), (40,), (100,), (10, 10), (40, 40), (100, 100)], 'learning_rate_init': [0.01, 0.05, 0.1,]}},\n",
    "    {\"param_grid\": {}},\n",
    "    {\"param_grid\": {'n_estimators':list(range(50,250,50)),'learning_rate':[0.01, 0.1, 0.2]}},\n",
    "    {\"param_grid\": {'C':[0.1,0.5,1,10], 'tol': [1e-2,1e-3,1e-4]}},\n",
    "    {\"param_grid\": {'base_estimator': [LogisticRegression()], 'n_estimators': [50,100,150,200]}}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross Tuples:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Sampling Strategy:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Classifiers:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Classifiers:  12%|█▎        | 1/8 [00:36<04:16, 36.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "Classifiers:  25%|██▌       | 2/8 [01:07<03:29, 34.86s/it]\u001b[A\u001b[A\n",
      "\n",
      "Classifiers:  38%|███▊      | 3/8 [01:10<02:06, 25.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "Classifiers:  50%|█████     | 4/8 [02:57<03:19, 49.93s/it]\u001b[A\u001b[A\n",
      "\n",
      "Classifiers:  62%|██████▎   | 5/8 [02:58<01:46, 35.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "Classifiers:  75%|███████▌  | 6/8 [03:24<01:04, 32.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "Classifiers:  88%|████████▊ | 7/8 [03:37<00:26, 26.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "Classifiers: 100%|██████████| 8/8 [04:02<00:00, 26.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "                                                          \u001b[A\u001b[A\n",
      "Sampling Strategy:  50%|█████     | 1/2 [04:02<04:02, 242.34s/it]\u001b[A\n",
      "\n",
      "Classifiers:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Classifiers:  12%|█▎        | 1/8 [00:25<03:00, 25.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "Classifiers:  25%|██▌       | 2/8 [00:53<02:38, 26.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "Classifiers:  38%|███▊      | 3/8 [00:55<01:35, 19.15s/it]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "# Warning: this step might take some time - skip to the next cell  \n",
    "# for a quick overview of the resulting models.\n",
    "df_results = u.train_and_eval(cross_tuples, classifiers, classifier_kwargs, undersample=True, save='results_anatomy-lb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR use previously ran models\n",
    "import pickle\n",
    "df_results = pickle.load(open(\"results_anatomy-lb.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results dataset contains the following information:\n",
    "- **name**: name given to the experiment\n",
    "- **measures**: the set of ontology matching tools that were used as features\n",
    "- **classifier**: pipeline used for the training of the models - object with the form ``Pipeline(sampling strategy, classifier)``\n",
    "- **testing_df**: contains the dataframe that was used for testing\n",
    "- **sampling_strategy**: the data balancement strategy used (either undersampling or oversampling)\n",
    "- **f1_train**: the f1-score obtained in the training phase (cross-evaluation). In this case, this corresponds to the evaluation within the``anatomy`` dataset.\n",
    "- **f1_test**: the f1-score obtained in the testing phase. In this case, this corresponds to the evaluation with the ``largebio`` dataset.\n",
    "- **acc_test**: the accuracy obtained in the testing phase.\n",
    "- **recall_test**: the recall obtained in the testing phase.\n",
    "- **precision_test**: the precision obtained in the testing phase.\n",
    "- **grid_search**:the parameters defined for the grid search and cross validation\n",
    "- **classifier_name**: the classifier object. This object could be used for the classification of novel instances (see how in section [4](#classification))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluating the models <a id = \"evaluation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.plot_top_results(df_results[df_results['name']=='anatomy-lb'],'Anatomy-LB',sort_by=['f1_test', 'f1_train'], top=3, logs = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot font and sizes\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)\n",
    "plt.rc('font', family='serif')\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(10,10))\n",
    "plt.subplots_adjust(wspace = 0.05)\n",
    "plt.ylim(0.7, 1.0)\n",
    "\n",
    "#Boxplots\n",
    "sns.boxplot(x=\"sampling_strategy\", \n",
    "            y=\"f1_train\", \n",
    "            data=df_results[df_results['name']=='anatomy-lb'], \n",
    "            ax=axes[0]).set(xlabel=None,ylabel='F1-scores',title='Train')\n",
    "\n",
    "#Boxplots\n",
    "sns.boxplot(x=\"sampling_strategy\", \n",
    "            y=\"f1_test\", \n",
    "            data=df_results[df_results['name']=='anatomy-lb'], \n",
    "            ax=axes[1]).set(xlabel=None,ylabel=None,title='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these results, we can conclude that the models trained with the ``Anatomy`` dataset were able to perform well within the same alignment (``Train F1-score``) but when tested against the ``LargeBio`` dataset, the performance was impaired. This is a sign of overfitting. Still, the overall F1-scores are high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Classifying a new instance <a id = \"classification\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = df_results['classifier_name'][11] #best classifier\n",
    "new_mapping_1 = [1, 0, 0, 0, 1, 0, 1, 0, 0] #pattern of tools' outputs\n",
    "new_mapping_2 = [1, 0, 0, 0, 0, 0, 1, 0, 1]\n",
    "\n",
    "print(\"The mapping\", new_mapping_1, \"is predicted to be: \", classifier.predict([new_mapping_1]))\n",
    "print(\"The mapping\", new_mapping_2, \" is predicted to be: \", classifier.predict([new_mapping_2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where ``0`` represents an incorrect mapping and ``1`` means a correct mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Authors\n",
    "Beatriz Lima, Cátia Pesquita\n",
    "    \n",
    "### License\n",
    "See the [LICENSE.md](https://github.com/liseda-lab/ML4ReferenceAlignment/blob/master/LICENSE) file for details.\n",
    "    \n",
    "### Acknowledgments\n",
    "This project was funded by the FCT through LASIGE Research Unit, ref. UIDB/00408/2020 and ref. UIDP/00408/2020, and by projects SMILAX (ref. PTDC/EEI-ESS/4633/2014)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
